# 🧱 Week 5 Learning Journal — Flask CRUD + Persistence
*Learning APIs Project*

---

## 🗓 Week Overview
**Goal:** Learn how APIs store, read, update, and delete data using Flask + SQLite + SQLAlchemy.

---

## 🧩 Day 1 — Persistence Foundations
**Focus:** What does “saving data” actually mean?

**What I did**
- Created project folder and virtual env  
- Installed Flask, SQLAlchemy, dotenv, pytest  
- Verified `/health` route  
- Fixed SQLite path errors with absolute path logic  

**What I learned**
- Persistence = data living beyond a single run  
- ORMs translate Python ↔ SQL so you don’t write raw queries  
- `commit()` makes data permanent  

**Challenges**
- “Unable to open database file” → fixed via `config.py` path builder  
- PowerShell curl alias confusion  

**Next Step**
- Implement Create + Read routes tomorrow

## 🧩 Day 2 — Create + Read Deep Dive

**Focus:** Adding validation and data integrity to the API.

**What I did**
- Enhanced `/records` POST to validate input fields (`city`, `temp`, `humidity`).
- Added consistent JSON error responses for invalid data.
- Tested different POST scenarios in PowerShell using `Invoke-WebRequest` and `Invoke-RestMethod`.

**What I learned**
- Validation is what separates hobby projects from production code.
- PowerShell and `curl.exe` handle JSON quoting differently — an important cross-platform lesson.
- Error responses should be predictable (always JSON, always include an `error` key).

**Challenges**
- Frustration with JSON quoting and `curl` syntax.
- Understanding why Flask rejected certain requests helped clarify how request bodies are parsed.
- The learning felt slow, but it reinforced core API behaviors (status codes, contracts).

**Next Step**
- Implement `PUT` and `DELETE` routes to complete the CRUD cycle.
- Add safeguards so updates and deletions are intentional and confirmed.

**Mood Check**
Slightly underwhelmed today — this part feels repetitive, but it’s the groundwork for professional-grade APIs. I’m getting more confident with Flask, SQLAlchemy, and structured testing.

## Day 3 — Update & Delete (CRUD U/D)

**What I built**
- Added `PUT /records/<id>` to update fields (`city`, `temp`, `humidity`).
- Added `DELETE /records/<id>` to remove a record.
- Added a global `@app.errorhandler(404)` so **all** 404s return JSON (no HTML pages).

**Why it matters**
- Clients need predictable error shapes. JSON 404s make programmatic handling easy.
- `db.session.commit()` is the “save point” for data—mirrors how `git commit` saves code.
- Idempotence: `DELETE` can be safely retried; `PUT` leads to a known final state.

**What I tested**
- Update success:
  - `PUT /records/1` with `{"city":"Seattle (Updated)","temp":19}` → `200` + updated JSON.
- Delete success:
  - `DELETE /records/<id>` for an existing id → `200` + confirmation message.
- Missing resource (defensive cases):
  - `PUT /records/999999` → `404` + `{"error":"Not Found"}` (or `{"error":"record not found"}` if from route).
  - `DELETE /records/999999` → `404` + JSON error.
- Verified collection:
  - `GET /records/` shows current rows.

**Mental model connections**
- CRUD in the API ↔ change management in Git:
  - Update data → `PUT` ↔ edit file → `git commit`
  - Delete data → `DELETE` ↔ `git rm` → `git commit`
- Route not found vs resource not found:
  - Route-miss 404 (framework) now returns JSON via the global handler.
  - Resource-miss 404 (handled in route) also returns JSON consistently.

**Gotchas I learned**
- Restart the dev server after editing routes (so Flask loads new code).
- Test with `curl -i` to see status + `Content-Type` and confirm JSON responses.

Mood check: generally pretty furstrated but mainly because git commands. 


## Day 4 — Stats, Seeding, SQL Thinking
- Wrote idempotent `scripts/seed.py` (safe to run multiple times).
- Verified `/records/stats` aggregates with `func.avg`, `func.count` (DB does the math).
- Added tests for empty + populated DB; configured pytest to find `app`.
- Takeaway: aggregate in SQL, serve in JSON; keep seeding reproducible.

## 🧱 Week 5 Day 5 — Logging & Error Hygiene  

**Goal:**  
Make the API production-ready by adding *visibility* (logs) and *predictability* (consistent JSON errors).

---

### 🧠 What I Learned  

- **Logging is observability.**  
  Every request now records its method, path, status, IP, and duration in `logs/app.log`.  
  I can finally *see* what’s happening inside my API—like a flight recorder for Flask.

- **Error consistency builds trust.**  
  Both `404` and `500` now return JSON objects instead of raw HTML, e.g.:  
  ```json
  {"error": "Not Found"}
  {"error": "Internal Server Error"}

---

## 🪞 Guided Reflection Addendum — Week 5 Day 5  

I didn’t feel totally confident answering the reflection questions at first.  
But after walking through what each idea *really means*, I can see how these concepts connect.

---

### 1️⃣ Why is logging critical once an API leaves your local machine?  
Because when my API runs on another computer or in the cloud, I can’t just `print()` things to see what’s happening.  
Logging is my visibility.  
It shows **who** hit my endpoints, **what** they did, and **how** my system responded.  
If something breaks or slows down, the logs are the story of what happened.  
They turn an invisible process into something observable and fixable.

---

### 2️⃣ What’s the danger of returning HTML error pages to JSON clients?  
APIs talk to code, not humans.  
If I send back an HTML error page, a front-end or automation client can’t parse it — it just fails.  
By always returning JSON like `{"error": "Not Found"}`, I make my API’s behavior predictable.  
That’s what lets other programs, mobile apps, or even future versions of my own app trust the interface.

---

### 3️⃣ How could structured logs (JSON format) make analytics easier later?  
Right now, my logs are readable by me, but not easily by machines.  
If I switched to structured JSON logs — one JSON object per line — I could feed them into a dashboard, filter them by route or response time, or visualize errors per day.  
That turns logs from “text to scan” into **data to analyze**.

---

### 4️⃣ How will I extend this when I deploy to a cloud host?  
Instead of writing to a file on disk, my Flask or FastAPI app would send logs to a cloud logger such as CloudWatch, Papertrail, or Supabase analytics.  
The exact tool changes, but the principles I practiced — rotation, structured messages, consistent error JSON — stay the same.  
Everything I built locally is the foundation for observability and stability in production.

---

### 🧭 Takeaway  
This week wasn’t just about CRUD.  
It was about learning to *see inside* my API — understanding that logs, errors, and tests are what separate **a script that works** from **a service that can be trusted**.  
I’m not just running code anymore; I’m running a system.

---

## 🗓️ What I Did
Today I introduced automated testing into my Flask CRUD app using `pytest` and Flask’s `test_client`.
At first, my delete test failed with a *“working outside of application context”* error — this forced me to understand how Flask manages database sessions through its **application context**.
I rewrote the tests to be **API-driven** (using real HTTP requests) instead of touching the database directly, and everything passed.

---

## 🔍 What I Learned

| Concept | Description |
|----------|--------------|
| **App Context** | Flask isolates resources like `db.session` inside an *application context*. Anything outside that context won’t know which app or request it belongs to. |
| **Flask Test Client** | Lets me call routes (`POST /records`, `DELETE /records/1`) inside tests without running the actual server. |
| **API-Driven Tests** | Simulate real client behavior instead of directly modifying the database. This is closer to how the API will be used in production. |
| **Test Isolation** | Each test starts with a clean database using `db.drop_all()` + `db.create_all()`, so results are consistent. |
| **Safety Net for Refactors** | With tests in place, I can change code confidently and prove that endpoints still behave correctly. |

---

## 🧩 Why It Matters
This was the first time I truly *trusted* my API.
Instead of manually cURLing every route, I can now run one command (`pytest -q`) and instantly see if anything broke.
It also built intuition for how frameworks like FastAPI or Django manage their own contexts — the same concepts will carry over.

Automated tests are what turn scripts into software.

---

## 💡 Key Takeaways
- Every route can be tested like a black-box system: send a request, assert on the JSON.  
- The application context is the invisible boundary that keeps Flask apps predictable.  
- Writing tests makes debugging 10× faster because failures point exactly to broken assumptions.  
- Good tests don’t just verify code — they **teach** you how your system behaves.

---

## 🔗 How It Connects
In future projects:
- **PathTwo** → verify financial inputs calculate the right savings rates.  
- **Recap Games** → test that recap generation endpoints return structured JSON before rendering UI.  
- **Garden Companion** → ensure POSTing a new plant actually persists across sessions.  

All of them will reuse the same pattern you built today:  
> `pytest + test_client → confidence + speed.`


## Week 5 - Day 6

## 🗓️ What I Did
Today I introduced automated testing into my Flask CRUD app using `pytest` and Flask’s `test_client`.
At first, my delete test failed with a *“working outside of application context”* error — this forced me to understand how Flask manages database sessions through its **application context**.
I rewrote the tests to be **API-driven** (using real HTTP requests) instead of touching the database directly, and everything passed.

---

## 🔍 What I Learned

| Concept | Description |
|----------|--------------|
| **App Context** | Flask isolates resources like `db.session` inside an *application context*. Anything outside that context won’t know which app or request it belongs to. |
| **Flask Test Client** | Lets me call routes (`POST /records`, `DELETE /records/1`) inside tests without running the actual server. |
| **API-Driven Tests** | Simulate real client behavior instead of directly modifying the database. This is closer to how the API will be used in production. |
| **Test Isolation** | Each test starts with a clean database using `db.drop_all()` + `db.create_all()`, so results are consistent. |
| **Safety Net for Refactors** | With tests in place, I can change code confidently and prove that endpoints still behave correctly. |

---

## 🧩 Why It Matters
This was the first time I truly *trusted* my API.
Instead of manually cURLing every route, I can now run one command (`pytest -q`) and instantly see if anything broke.
It also built intuition for how frameworks like FastAPI or Django manage their own contexts — the same concepts will carry over.

Automated tests are what turn scripts into software.

---

## 💡 Key Takeaways
- Every route can be tested like a black-box system: send a request, assert on the JSON.  
- The application context is the invisible boundary that keeps Flask apps predictable.  
- Writing tests makes debugging 10× faster because failures point exactly to broken assumptions.  
- Good tests don’t just verify code — they **teach** you how your system behaves.

---

## 🔗 How It Connects
In future projects:
- **PathTwo** → verify financial inputs calculate the right savings rates.  
- **Recap Games** → test that recap generation endpoints return structured JSON before rendering UI.  
- **Garden Companion** → ensure POSTing a new plant actually persists across sessions.  

All of them will reuse the same pattern you built today:  
> `pytest + test_client → confidence + speed.`
